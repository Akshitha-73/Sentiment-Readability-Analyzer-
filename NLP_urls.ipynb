{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YT6lhQB4SWs8",
        "outputId": "03903f37-4aa1-4b74-f3ef-b31e6f711206"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import csv\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Extraction"
      ],
      "metadata": {
        "id": "Q2Zq7X13gRzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_url(url):\n",
        "  try:\n",
        "    res=requests.get(url,timeout=10)\n",
        "    if res.status_code==200:\n",
        "      soup=BeautifulSoup(res.text,'html.parser')\n",
        "      if soup.title:\n",
        "        title=soup.title.string.strip()\n",
        "      else:\n",
        "        print(\"No Title Found\")\n",
        "      content=soup.find('article')\n",
        "\n",
        "      if content:\n",
        "        content_text=content.get_text(separator=' ').strip()\n",
        "      else:\n",
        "        print(\"No Content Found\")\n",
        "\n",
        "      return title,content_text\n",
        "\n",
        "    else:\n",
        "      return \"Error\",f\"Failed to fetch content. Status code: {res.status_code}\"\n",
        "\n",
        "  except Exception as e:\n",
        "    return \"Error\",f\"Error occurred: {str(e)}\""
      ],
      "metadata": {
        "id": "bVk0ZqLvznhC"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text=text.lower()\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text"
      ],
      "metadata": {
        "id": "AplkY2LW6EqZ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_excel('/content/Input.xlsx')\n",
        "results=[]\n",
        "for _, row in df.iterrows():\n",
        "  url_id=row['URL_ID']\n",
        "  url=row['URL']\n",
        "  title,content=scrape_url(url)\n",
        "\n",
        "  cleaned_content=clean_text(content)\n",
        "  results.append({\"URL_ID\":url_id,\"URL\":url,\"Title\":title,\"Cleaned Content\":cleaned_content})"
      ],
      "metadata": {
        "id": "zFQuvQN7UkoT"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1LIMMuJUvlV",
        "outputId": "42aa8f05-4cf5-4c8c-819b-b26f2113dc97"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'URL_ID': 'Netclan20241018',\n",
              " 'URL': 'https://insights.blackcoffer.com/enhancing-front-end-features-and-functionality-for-improved-user-experience-and-dashboard-accuracy-in-partner-hospital-application/',\n",
              " 'Title': 'Enhancing Front-End Features and Functionality for Improved User Experience and Dashboard Accuracy in Partner Hospital Application | Blackcoffer Insights',\n",
              " 'Cleaned Content': 'home our success stories enhancing frontend features and functionality for improved user experience and dashboard accuracy our success stories healthcare enhancing frontend features and functionality for improved user experience and dashboard accuracy in partner hospital application by ajay bidyarthy august 26 2024 10415 client background client a leading hospital chain in the usa industry type healthcare products services healthcare solutions organization size 200 the problem build a web application develop deploy and maintain the system in the background and there will be more hospitals partnering with our organization utilizing this tool per our service offering which could lead to longterm working contracts with us if interested the current project is a hippacompliant spa web application that will interface with hospital data dashboards involving discharged patients callnavigator utilizes an sftp server that is hosted in aws this allows our partner hospital to store patient discharge files directly to an s3 bucket the application and dashboard are currently up and running and pulling information and data from our partner hospital however the project was rushed through initial development and there are several frontend features that need to be addressed or finetunedupgraded and functions added searchflag options and redirection to make the application more userfriendly dashboard accuracy there are deliverables such as reporting that is part of the project scope that was not developed but are considered a priority after the features and functionality are improved according to the og developer a github account for our organization will need to be created and code uploaded to two separate projects one for backend and one for frontend let me know if this project would be something you would like to further discuss thank you so much for your time and consideration more information from the og developer follows developer should specifically know or be able to learn these specific aws features since they will be responsible for deployment and maintaining the system for deployment the developer must modify aws security groups and target groups etc push code to product do not remove og developer accessnetwork connection currently there are github actions created that will deploy the code if you set up the proper secrets github actions on feangular app with deploy code to our static s3 website as well as creations invalidations for cloudfront that bypasses the caching an ssh config link and key file are available need to be able to pull reports on call info by unit by caller by diagnosis physician or across the hospital specifically on flags for medications conditions ect flags and marked issues also need to be represented on the facility dashboard page automate the import process of file from hospital set up or ability to set up hospital employees to log in with their blessing credentials single sign on flags thrown need to show up somewhere in call navigator and also be able to go into them and view patients and issues right now the only notification and way to view this is strictly through email ability to hover over icons on facility dashboard and see info and be able to click on info and have a page with that info pop up on the discharge tab when setting a filter we want that filter to stay set unless we change it right now if you set a filter and then go into a patient when you come back out the filter is not set anymore and you have to set all over again our solution need to be able to pull reports on call info by unit by caller by diagnosis physician or across the hospital specifically on flags for medications conditions ect flags and marked issues also need to be represented on the facility dashboard page automate the import process of file from hospital set up or ability to set up hospital employees to log in with their blessing credentials single sign on flags thrown need to show up somewhere in call navigator and also be able to go into them and view patients and issues right now the only notification and way to view this is strictly through email ability to hover over icons on facility dashboard and see info and be able to click on info and have a page with that info pop up on the discharge tab when setting a filter we want that filter to stay set unless we change it right now if you set a filter and then go into a patient when you come back out the filter is not set anymore and you have to set all over again also let me know if there is a scope of project document that we need to complete to help you and your team develop a project plan and timeline hopefully with the above notes and attached files youll be able to put together a project plan and proposal for this project deliverables fully functional application product maintenance and support tech stack tools used vs code languagetechniques used angular nodejs expressjs databases used mysql web cloud servers used aws project snapshots project website url httpscallsnavigatorcom summarize summarized httpsblackcoffercom this project was done by the blackcoffer team a global it consulting firm contact details this solution was designed and developed by blackcoffer team here are my contact details firm name blackcoffer pvt ltd firm website wwwblackcoffercom firm address 42 eextension shaym vihar phase 1 new delhi 110043 email ajayblackcoffercom skype asbidyarthy whatsapp 91 9717367468 telegram asbidyarthy previous article roas dashboard for campaignwise google ads budget tracking using google ads ap next article ai and mlbased youtube analytics and content creation tool for optimizing subscriber engagement and content strategy ajay bidyarthy related articles more from author from complexity to clarity transforming data into decisions through mixed modelling aws codepipeline is utilized for automatically building and deploying lambda functions in aws dockerize the aws lambda for serverless architecture'}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_urls(input,output_folder):\n",
        "  if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "output_folder = '/content'\n",
        "process_urls(input, output_folder)\n",
        "\n",
        "csv_file_path=os.path.join(output_folder,'processed_urls.csv')\n",
        "\n",
        "with open(csv_file_path,'w',newline='',encoding='utf-8') as csvfile:\n",
        "  filenames=['URL_ID','URL','Title','Cleaned Content']\n",
        "  writer=csv.DictWriter(csvfile,fieldnames=filenames)\n",
        "\n",
        "  writer.writeheader()\n",
        "  for i in results:\n",
        "    writer.writerow(i)\n",
        "\n",
        "print(f\"Processed URLs have been saved to {csv_file_path}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m2masvObVEB4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cac6f12e-71ef-4ab4-ff8f-fe744a2200a6"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed URLs have been saved to /content/processed_urls.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ngqe4HxwSfjn",
        "outputId": "5dbb3d5c-08e9-47bb-b601-f12d3d8307eb"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cleaning data"
      ],
      "metadata": {
        "id": "C43foO5JgeTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile"
      ],
      "metadata": {
        "id": "yJDndMjyDNO6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_from_zip(zip_path):\n",
        "    word_set = set()\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        for filename in zip_ref.namelist():\n",
        "            if filename.endswith('.txt'):\n",
        "                with zip_ref.open(filename) as file:\n",
        "                    content = file.read().decode('latin-1')\n",
        "                    for line in content.splitlines():\n",
        "                        word = line.strip().lower()\n",
        "                        if word and not word.startswith(('#', '//')):\n",
        "                            word_set.add(word)\n",
        "    return word_set"
      ],
      "metadata": {
        "id": "yP8yn38DdUO3"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_set = extract_from_zip('/content/StopWords.zip')\n"
      ],
      "metadata": {
        "id": "kq934m88z7d7"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(stopwords_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sBXFv2qV2EcK",
        "outputId": "ace66b03-cb69-438d-8b51-7071c5a34369"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12768"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_sentiment_dictionaries(stopwords_zip, master_dict_zip):\n",
        "\n",
        "    stopwords_set = extract_from_zip(stopwords_zip)\n",
        "    positive_set = set()\n",
        "    negative_set = set()\n",
        "\n",
        "    with zipfile.ZipFile(master_dict_zip, 'r') as zip_ref:\n",
        "        for filename in zip_ref.namelist():\n",
        "            if filename.lower().endswith('positive-words.txt'):\n",
        "                with zip_ref.open(filename) as file:\n",
        "                    for line in file.read().decode('latin-1').splitlines():\n",
        "                        word = line.strip().lower()\n",
        "                        if word and not word.startswith(('#', '//')) and word not in stopwords_set:\n",
        "                            positive_set.add(word)\n",
        "\n",
        "            elif filename.lower().endswith('negative-words.txt'):\n",
        "                with zip_ref.open(filename) as file:\n",
        "                    for line in file.read().decode('latin-1').splitlines():\n",
        "                        word = line.strip().lower()\n",
        "                        if word and not word.startswith(('#', '//')) and word not in stopwords_set:\n",
        "                            negative_set.add(word)\n",
        "\n",
        "    return stopwords_set, positive_set, negative_set\n",
        "\n"
      ],
      "metadata": {
        "id": "IBPw8LQQd1xP"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_set, positive_set, negative_set = prepare_sentiment_dictionaries('/content/StopWords.zip',\n",
        "                                                                           '/content/MasterDictionary .zip')"
      ],
      "metadata": {
        "id": "AVQGXSit24H1"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polarity →\n",
        "\n",
        "```\n",
        "polarity\n",
        "=\n",
        "(positive count  − negative count)/\n",
        " (positive count  +  negative count)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Gives a range from -1 (very negative) to +1 (very positive).\n",
        "\n",
        "Subjectivity →\n",
        "```\n",
        "subjectivity\n",
        "=\n",
        "(positive count +  negative count)/\n",
        "(total words without stopwords)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "Higher value means more opinionated text.\n",
        "\n",
        "Sentiment Score →\n",
        "You can define as\n",
        "\n",
        "``` (positive count - negative count) ```\n",
        "\n",
        " (a raw strength measure)."
      ],
      "metadata": {
        "id": "1n_otB3e87gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_text(content, stop_words):\n",
        "    words = word_tokenize(content)\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return filtered_words"
      ],
      "metadata": {
        "id": "FXFF58g0-d6_"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_sentiment(words, positive_words, negative_words):\n",
        "    positive_count = sum(1 for word in words if word.lower() in positive_words)\n",
        "    negative_count = sum(1 for word in words if word.lower() in negative_words)\n",
        "    polarity = (positive_count - negative_count) / ((positive_count + negative_count))\n",
        "    subjectivity = (positive_count + negative_count) / ((len(words)))\n",
        "    sentiment_score = positive_count - negative_count\n",
        "    return {\n",
        "        \"positive_count\": positive_count,\n",
        "        \"negative_count\": negative_count,\n",
        "        \"polarity\": polarity,\n",
        "        \"subjectivity\": subjectivity,\n",
        "        \"sentiment_score\": sentiment_score\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "id": "rHrUiJCXSfeV"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentimental Analysis"
      ],
      "metadata": {
        "id": "x8Td7sGsgkYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process(input_file, stop_words, positive_words, negative_words):\n",
        "    df = pd.read_csv(input_file)\n",
        "    results = []\n",
        "    for index, row in df.iterrows():\n",
        "        content = row.get(\"Cleaned Content\")\n",
        "        if pd.notnull(content):\n",
        "          filtered_words = filter_text(content, stop_words)\n",
        "          sentiment = analyze_sentiment(filtered_words, positive_words, negative_words)\n",
        "\n",
        "          results.append({\n",
        "                \"URL_ID\": row['URL_ID'],\n",
        "                \"positive_count\": sentiment[\"positive_count\"],\n",
        "                \"negative_count\": sentiment[\"negative_count\"],\n",
        "                \"polarity\": sentiment[\"polarity\"],\n",
        "                \"subjectivity\": sentiment[\"subjectivity\"],\n",
        "                \"sentiment_score\": sentiment[\"sentiment_score\"]\n",
        "            })\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "SEsHKb-gSfbz"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = '/content/processed_urls.csv'\n",
        "output_csv_file = '/content/sentiment_analysis_results.csv'\n",
        "\n",
        "results = process(input_file, stopwords_set,positive_set, negative_set)"
      ],
      "metadata": {
        "id": "UKK4cMkd-vd0"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(output_csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    fieldnames = [\"URL_ID\", \"positive_count\", \"negative_count\", \"polarity\", \"subjectivity\", \"sentiment_score\"]\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writeheader()\n",
        "    for result in results:\n",
        "        writer.writerow(result)\n",
        "\n",
        "print(f\"Sentiment analysis results saved to {output_csv_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnSkG7ke_GZr",
        "outputId": "6eb49080-d360-4c65-9935-3025f52f27b9"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment analysis results saved to /content/sentiment_analysis_results.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis of Readability\n"
      ],
      "metadata": {
        "id": "2IC-QcIMgWJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk import download\n",
        "download('punkt')\n",
        "download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxDqrwHqaljg",
        "outputId": "cd07261e-7aae-43b4-e15b-5acb362780e4"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_syllables(word):\n",
        "    word = word.lower()\n",
        "    vowels = \"aeiou\"\n",
        "    count = 0\n",
        "    prev_char_was_vowel = False\n",
        "    for char in word:\n",
        "        if char in vowels:\n",
        "            if not prev_char_was_vowel:\n",
        "                count += 1\n",
        "            prev_char_was_vowel = True\n",
        "        else:\n",
        "            prev_char_was_vowel = False\n",
        "    if word.endswith(\"e\") and count > 1:\n",
        "        count -= 1\n",
        "    return count\n"
      ],
      "metadata": {
        "id": "PTjZgmBnCleS"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_readability_metrics(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    num_sentences = len(sentences)\n",
        "    num_words = len(words)\n",
        "\n",
        "    complex_words = [i for i in words if count_syllables(i) >= 3]\n",
        "    num_complex_words = len(complex_words)\n",
        "\n",
        "    average_sentence_length = num_words / num_sentences if num_sentences > 0 else 0\n",
        "\n",
        "    percentage_complex_words = (num_complex_words / num_words) * 100 if num_words > 0 else 0\n",
        "\n",
        "    fog_index = 0.4 * (average_sentence_length + percentage_complex_words)\n",
        "\n",
        "\n",
        "    return average_sentence_length, percentage_complex_words, fog_index\n"
      ],
      "metadata": {
        "id": "1ussGHEmd7Ac"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = '/content/processed_urls.csv'\n",
        "output_file = '/content/readability_metrics.csv'\n",
        "\n",
        "df = pd.read_csv(input_file)\n",
        "results = []\n",
        "for _, row in df.iterrows():\n",
        "    content = row.get(\"Cleaned Content\")\n",
        "    if pd.notnull(content):\n",
        "        avg_sentence_length, pct_complex_words, fog_idx = compute_readability_metrics(content)\n",
        "        results.append({\n",
        "            \"URL_ID\": row.get(\"URL_ID\"),  # Ensure URL_ID is preserved\n",
        "            \"Average Sentence Length\": avg_sentence_length,\n",
        "            \"Percentage of Complex Words\": pct_complex_words,\n",
        "            \"Fog Index\": fog_idx\n",
        "        })"
      ],
      "metadata": {
        "id": "VyQ-a8W1DgwD"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    fieldnames = [\"URL_ID\", \"Average Sentence Length\", \"Percentage of Complex Words\", \"Fog Index\"]\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writeheader()\n",
        "    for result in results:\n",
        "        writer.writerow(result)\n",
        "\n",
        "print(f\"Readability metrics saved to {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEeXzRSBEpMI",
        "outputId": "b9b6fced-4f0d-4bb5-f098-dc2e5f98d9f8"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Readability metrics saved to /content/readability_metrics.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Average Number of Words Per Sentence"
      ],
      "metadata": {
        "id": "AvMsrSCLeVf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(text):\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    total_words = len(words)\n",
        "\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    total_sentences = len([s for s in sentences if s.strip()])\n",
        "\n",
        "    average_words_per_sentence = total_words / total_sentences if total_sentences else 0\n",
        "\n",
        "    return total_words, total_sentences, average_words_per_sentence"
      ],
      "metadata": {
        "id": "MKaB-ZeLefvD"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = '/content/processed_urls.csv'\n",
        "output_file = '/content/average_words_per_sentence.csv'\n",
        "\n",
        "df = pd.read_csv(input_file)\n",
        "results = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    content = row.get(\"Cleaned Content\")\n",
        "    if pd.notnull(content):\n",
        "        total_words, total_sentences, avg_words_per_sentence = calculate_metrics(content)\n",
        "\n",
        "        results.append({\n",
        "            \"URL_ID\": row.get(\"URL_ID\"),\n",
        "            \"Total Words\": total_words,\n",
        "            \"Total Sentences\": total_sentences,\n",
        "            \"Average Words Per Sentence\": avg_words_per_sentence\n",
        "        })"
      ],
      "metadata": {
        "id": "ifG3JQsQFgcL"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_df = pd.DataFrame(results)\n",
        "output_df.to_csv(output_file, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"Metrics saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_691hXAFuC9",
        "outputId": "89df747d-917f-44a1-e0e2-f0148481315f"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics saved to /content/average_words_per_sentence.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## complex count"
      ],
      "metadata": {
        "id": "DSZNYQI2f75z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def complex_words(text):\n",
        "    words = word_tokenize(text)\n",
        "    complex_words = [w for w in words if count_syllables(w) >= 3]\n",
        "    return len(complex_words)"
      ],
      "metadata": {
        "id": "E5N4AtwXG-M7"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = '/content/processed_urls.csv'\n",
        "output_file = '/content/complex_word_count.csv'\n",
        "\n",
        "df = pd.read_csv(input_file)\n",
        "results = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    content = row.get(\"Cleaned Content\")\n",
        "    if pd.notnull(content):\n",
        "        complex_word_count = complex_words(content)\n",
        "        results.append({\n",
        "            \"URL_ID\": row.get(\"URL_ID\"),\n",
        "            \"Complex Word Count\": complex_word_count\n",
        "        })\n"
      ],
      "metadata": {
        "id": "iDImKJPPefsb"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_df = pd.DataFrame(results)\n",
        "output_df.to_csv(output_file, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"Complex word count results saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_ApPDrhHijm",
        "outputId": "23e7c699-6ceb-4d91-adc7-4f8025b24d2e"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complex word count results saved to /content/complex_word_count.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## word count"
      ],
      "metadata": {
        "id": "rR8OibcfgXF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def count_cleaned_words(text):\n",
        "    words = word_tokenize(text)\n",
        "    cleaned_words = [re.sub(r'[^\\w\\s]', '', word.lower()) for word in words if word.lower() not in stop_words]\n",
        "    cleaned_words = [word for word in cleaned_words if word]\n",
        "    return len(cleaned_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvESWmXKefqb",
        "outputId": "04fc665a-cef6-4d89-9ef6-8573aa03c572"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = '/content/processed_urls.csv'\n",
        "output_file = '/content/word_count_results.csv'\n",
        "\n",
        "df = pd.read_csv(input_file)\n",
        "results = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    content = row.get(\"Cleaned Content\")\n",
        "    if pd.notnull(content):\n",
        "        word_count = count_cleaned_words(content)\n",
        "        results.append({\n",
        "            \"URL_ID\": row.get(\"URL_ID\"),\n",
        "            \"Cleaned Word Count\": word_count\n",
        "        })"
      ],
      "metadata": {
        "id": "JRccPVRRH-OL"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_df = pd.DataFrame(results)\n",
        "output_df.to_csv(output_file, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"Word count results saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwWYX0stH_ri",
        "outputId": "44f875a4-aba4-47df-a9c8-64bcd2911ab8"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word count results saved to /content/word_count_results.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## count syllables"
      ],
      "metadata": {
        "id": "1Qu78D6Zk1Ao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_syllables(word):\n",
        "    word = word.lower()\n",
        "    vowels = \"aeiou\"\n",
        "    syllable_count = len(re.findall(r'[aeiou]+', word))\n",
        "    if word.endswith(\"es\") or word.endswith(\"ed\"):\n",
        "        if len(word) > 2 and not re.search(r'[aeiou]', word[:-2]):\n",
        "            syllable_count -= 1\n",
        "    return max(1, syllable_count)\n",
        "\n",
        "def analyze_syllables(text):\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    syllable_counts = [count_syllables(word) for word in words]\n",
        "    return sum(syllable_counts), syllable_counts"
      ],
      "metadata": {
        "id": "d6oB5kvBefop"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = '/content/processed_urls.csv'\n",
        "output_file = '/content/syllable_count_results.csv'\n",
        "\n",
        "df = pd.read_csv(input_file)\n",
        "results = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    content = row.get(\"Cleaned Content\")\n",
        "    if pd.notnull(content):\n",
        "        total_syllables, syllable_list = analyze_syllables(content)\n",
        "        results.append({\n",
        "            \"URL_ID\": row.get(\"URL_ID\"),\n",
        "            \"Total Syllables\": total_syllables,\n",
        "            \"Average Syllables Per Word\": total_syllables / len(syllable_list) if syllable_list else 0\n",
        "        })\n"
      ],
      "metadata": {
        "id": "Kni1LAydJKX7"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_df = pd.DataFrame(results)\n",
        "output_df.to_csv(output_file, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"Syllable count results saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-N5Za6CeJULM",
        "outputId": "6107b3dc-c79f-4dbb-c526-4ea874c36f48"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Syllable count results saved to /content/syllable_count_results.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## count personal pronouns"
      ],
      "metadata": {
        "id": "5v2qzPzrljmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_personal_pronouns(text):\n",
        "    pronoun_pattern = r'\\b(I|we|my|ours|us)\\b'\n",
        "    matches = re.findall(pronoun_pattern, text, flags=re.IGNORECASE)\n",
        "    matches = [i for i in matches if i.lower() != \"us\"]\n",
        "    return len(matches)"
      ],
      "metadata": {
        "id": "tsOVmof8efmo"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = \"/content/processed_urls.csv\"\n",
        "output_file = \"/content/personal_pronouns_analysis.csv\"\n",
        "\n",
        "df = pd.read_csv(input_file)\n",
        "\n",
        "df['Personal Pronouns Count'] = df['Cleaned Content'].apply(count_personal_pronouns)\n",
        "\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Personal pronouns analysis saved to {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR991RXST-YS",
        "outputId": "37189999-ba52-4bdd-b048-f74bbef8012a"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Personal pronouns analysis saved to /content/personal_pronouns_analysis.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## average word length"
      ],
      "metadata": {
        "id": "ZQ6EAFObmvOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_average_word_length(text):\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    total_characters = sum(len(word) for word in words)\n",
        "    total_words = len(words)\n",
        "    if total_words == 0:\n",
        "        return 0\n",
        "\n",
        "    return total_characters / total_words"
      ],
      "metadata": {
        "id": "rQJizXoqefjw"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = \"/content/processed_urls.csv\"\n",
        "df['Average Word Length'] = df['Cleaned Content'].apply(calculate_average_word_length)\n",
        "\n",
        "output_file = \"/content/average_word_length_analysis.csv\"\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Average word length analysis saved to {output_file}\")"
      ],
      "metadata": {
        "id": "vpQ_ND2Jefhb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd13bb5a-4fed-4f45-c020-143a4fff3432"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average word length analysis saved to /content/average_word_length_analysis.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_file = '/content/sentiment_analysis_results.csv'\n",
        "readability_file = '/content/readability_metrics.csv'\n",
        "avg_words_file = '/content/average_words_per_sentence.csv'\n",
        "complex_word_count_file = '/content/complex_word_count.csv'\n",
        "word_count_file = '/content/word_count_results.csv'\n",
        "syllable_count_file = '/content/syllable_count_results.csv'\n",
        "average_word_length_file = '/content/average_word_length_analysis.csv'"
      ],
      "metadata": {
        "id": "Qkhof9VlU-0_"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sentiment = pd.read_csv(sentiment_file)\n",
        "df_readability = pd.read_csv(readability_file)\n",
        "df_avg_words = pd.read_csv(avg_words_file)\n",
        "df_complex_word= pd.read_csv(complex_word_count_file)\n",
        "df_word_count = pd.read_csv(word_count_file)\n",
        "df_syllable_count = pd.read_csv(syllable_count_file)\n",
        "df_average_word_length=pd.read_csv(average_word_length_file)\n",
        "\n"
      ],
      "metadata": {
        "id": "P_EB_wTRVwjR"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df = df_sentiment.merge(df_readability, on=\"URL_ID\", how=\"outer\")\n",
        "combined_df = combined_df.merge(df_avg_words, on=\"URL_ID\", how=\"outer\")\n",
        "combined_df= combined_df.merge(df_complex_word, on=\"URL_ID\", how=\"outer\")\n",
        "combined_df = combined_df.merge(df_word_count, on=\"URL_ID\", how=\"outer\")\n",
        "combined_df = combined_df.merge(df_syllable_count, on=\"URL_ID\", how=\"outer\")\n",
        "combined_df = combined_df.merge(df_average_word_length, on=\"URL_ID\", how=\"outer\")"
      ],
      "metadata": {
        "id": "hPWlh8WnV3t0"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_combined = '/content/final_combined_results.csv'\n"
      ],
      "metadata": {
        "id": "oiZxp2f6V9yy"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = [\"URL_ID\", \"URL\", \"Title\",\"Cleaned Content\"] + [c for c in combined_df.columns if c not in [\"URL_ID\", \"URL\", \"Title\",\"Cleaned Content\"]]\n",
        "combined_df = combined_df[cols]\n"
      ],
      "metadata": {
        "id": "dGertRKVXc-j"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df.to_csv(output_combined, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"Combined results saved to {output_combined}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWVkc2NuW8uG",
        "outputId": "297e2774-5e87-4cae-8cb9-8cf82b026795"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined results saved to /content/final_combined_results.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kXMmdMttXxlf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}